# -*- coding: utf-8 -*-
"""Lab10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GL9nbigwtQH8cJV15R_CQ6Q1p50U5Jg2
"""

from google.colab import files
files.upload()  # upload kaggle.json here
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d kutaykutlu/drowsiness-detection
!unzip drowsiness-detection.zip -d drowsiness_dataset

"""A1"""

# üì¶ Import Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# üìÅ Load Dataset
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    'drowsiness_dataset',
    target_size=(64, 64),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    'drowsiness_dataset',
    target_size=(64, 64),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

# ‚öôÔ∏è CNN Model Definition with Parametrized Regularization
def build_model(l2_value=0.01, dropout_rate=0.5, optimizer_name='adam'):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(l2_value), input_shape=(64, 64, 3)),
        BatchNormalization(),
        MaxPooling2D(2, 2),

        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(l2_value)),
        BatchNormalization(),
        MaxPooling2D(2, 2),

        Flatten(),
        Dense(128, activation='relu', kernel_regularizer=l2(l2_value)),
        Dropout(dropout_rate),
        BatchNormalization(),

        Dense(1, activation='sigmoid')  # Binary classification
    ])

    model.compile(optimizer=optimizer_name, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# üîÅ Experiment with different values
l2_values = [0.001, 0.005, 0.01]
dropout_rates = [0.3, 0.5, 0.7]
optimizers = ['adam', 'sgd', 'rmsprop', 'adagrad', 'adamax', 'nadam']

results = []

for l2_val in l2_values:
    for dropout in dropout_rates:
        for opt in optimizers:
            print(f"\nüîß Training with L2={l2_val}, Dropout={dropout}, Optimizer={opt.upper()}")
            model = build_model(l2_value=l2_val, dropout_rate=dropout, optimizer_name=opt)
            history = model.fit(train_generator, epochs=10, validation_data=val_generator, verbose=0)

            y_true = val_generator.classes
            y_pred_probs = model.predict(val_generator, verbose=0)
            y_pred = (y_pred_probs > 0.5).astype('int').flatten()

            acc = np.mean(y_pred == y_true)
            prec = precision_score(y_true, y_pred)
            rec = recall_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred)

            results.append({
                'l2': l2_val,
                'dropout': dropout,
                'optimizer': opt,
                'accuracy': acc,
                'precision': prec,
                'recall': rec,
                'f1_score': f1,
                'train_loss': history.history['loss'][-1],
                'val_loss': history.history['val_loss'][-1]
            })

            # üìä Plot Confusion Matrix
            cm = confusion_matrix(y_true, y_pred)
            plt.figure(figsize=(5, 4))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Closed', 'Open'], yticklabels=['Closed', 'Open'])
            plt.title(f'Confusion Matrix (Opt: {opt.upper()}, L2: {l2_val}, Drop: {dropout})')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.show()

# üìã Summary Table
print("\nüîç Experiment Summary:")
for r in results:
    print(f"L2: {r['l2']}, Dropout: {r['dropout']}, Optimizer: {r['optimizer'].upper()} ‚û§ "
          f"Acc: {r['accuracy']:.4f}, Prec: {r['precision']:.4f}, Rec: {r['recall']:.4f}, F1: {r['f1_score']:.4f}")

"""A2"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# Dummy sequence data: 100 samples, 10 timesteps, 64 features
X_seq = np.random.rand(100, 10, 64)
y_seq = np.random.randint(0, 2, 100)

# Build RNN
rnn_model = Sequential([
    SimpleRNN(64, activation='relu', input_shape=(10, 64)),
    Dense(1, activation='sigmoid')
])

rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.summary()

# Train
rnn_model.fit(X_seq, y_seq, epochs=5, batch_size=16)